{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /home/robert/anaconda3/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/robert/anaconda3/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/robert/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.5.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /home/robert/anaconda3/lib/python3.8/site-packages (1.2.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/robert/.local/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/robert/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/robert/.local/lib/python3.8/site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/robert/.local/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imbalanced-learn in /home/robert/anaconda3/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/robert/.local/lib/python3.8/site-packages (from imbalanced-learn) (1.19.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/robert/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.5.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/robert/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/robert/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /home/robert/anaconda3/lib/python3.8/site-packages (0.11.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/robert/anaconda3/lib/python3.8/site-packages (from seaborn) (3.2.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/robert/anaconda3/lib/python3.8/site-packages (from seaborn) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/robert/.local/lib/python3.8/site-packages (from seaborn) (1.19.1)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/robert/anaconda3/lib/python3.8/site-packages (from seaborn) (1.2.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/robert/.local/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/robert/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/robert/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/robert/anaconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/robert/.local/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/robert/.local/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scipy in /home/robert/anaconda3/lib/python3.8/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/robert/.local/lib/python3.8/site-packages (from scipy) (1.19.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install directly into notebook\n",
    "%pip install sklearn\n",
    "%pip install pandas\n",
    "%pip install imbalanced-learn\n",
    "%pip install seaborn\n",
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider making list of dependencies for TA to install when running this notebook\n",
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, KFold, cross_validate\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warning for chained assignment (not necessary but cleans up the project)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from kaggle example\n",
    "class ReplaceZeroTransformer():\n",
    "    \"\"\"Eliminates Zero values from tempo columns and replace it \n",
    "       with the median or mean of non-zero values as specified.\n",
    "       defaut is set to 'median'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='median'):\n",
    "        self.method = method\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.method == 'median':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].median()\n",
    "        elif self.method == 'mean':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].mean()\n",
    "        else:\n",
    "            raise Exception(\"Method can be 'median' or 'mean' only!\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import track data\n",
    "usecols = ['acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode','popularity', 'speechiness', 'tempo', 'valence']\n",
    "dataset = pd.read_csv(\"data.csv\", header = 0, usecols=usecols)\n",
    "\n",
    "# Remove rows duplicated by ignoring some columns\n",
    "dataset = dataset[~dataset.duplicated()==1]\n",
    "\n",
    "# Normalize columns having values outside [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "# cols_to_normalize = ['duration_ms', 'key', 'loudness', 'popularity', 'tempo']\n",
    "cols_to_normalize = ['duration_ms', 'loudness', 'tempo']\n",
    "dataset[cols_to_normalize] = scaler.fit_transform(dataset[cols_to_normalize])\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data analysis\n",
    "# pair plots: https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "import seaborn as sb\n",
    "\n",
    "# Pairwise plots between each column in the data set excluding popularity, binary (mode and explicit) and discrete (key) features\n",
    "pair_plot_data = dataset.drop(columns=['mode', 'explicit', 'key', 'popularity'])\n",
    "sb.pairplot(pair_plot_data, corner=True)\n",
    "# There isn't a lot of direct correlation between any two property of a song\n",
    "\n",
    "# Correlation between each property and popularity\n",
    "sb.pairplot(dataset, x_vars=usecols, y_vars=['popularity'])\n",
    "# The proprties we have do not directly correlate with the popularity of a song\n",
    "\n",
    "# Analyze binary and discrete features (mode, explicit, key)\n",
    "sb.displot(data=dataset, x='mode')        # more songs are in major key\n",
    "sb.displot(data=dataset, x='explicit')    # most songs are non explicit, most likely do not need this feature\n",
    "sb.displot(data=dataset, x='key')         # somewhat even distribution of keys\n",
    "\n",
    "# Analyze features in relation to popularity; these graphs are not very useful\n",
    "sb.pairplot(dataset, y_vars=['mode', 'explicit', 'key'], x_vars=['popularity'])\n",
    "\n",
    "# Count non-continuous features\n",
    "non_explicit = dataset[dataset['explicit'] == 0].shape[0]\n",
    "major = dataset[dataset['mode'] == 1].shape[0]\n",
    "total_songs = dataset.shape[0]\n",
    "print(f\"Non-Explicit songs / total songs = {non_explicit/total_songs}\")\n",
    "print(f\"Major key songs / total songs = {major/total_songs}\")\n",
    "\n",
    "# Analyze range of popularity of songs given explicit = 0 or 1 and mode = 0 or 1\n",
    "sb.histplot(data=dataset, x=\"popularity\", hue=\"explicit\", multiple=\"dodge\")\n",
    "# sb.histplot(data=dataset, x=\"popularity\", hue=\"mode\", multiple=\"dodge\")    # similar shapes for both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: further preprocessing?\n",
    "\n",
    "y = dataset.pop('popularity') # popularity is our class to predict\n",
    "X_headers = list(dataset.columns.values)\n",
    "X = dataset\n",
    "\n",
    "# Create the under sampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "# apply the transform\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "tempo_transformer = ReplaceZeroTransformer()\n",
    "X = tempo_transformer.transform(X)\n",
    "\n",
    "# need to scale after to treat the individual categories as their own class for the undersampling\n",
    "y = y/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the keys since they are a multiclass\n",
    "ohe = OneHotEncoder(categories='auto', drop='first')\n",
    "\n",
    "feature_arr = ohe.fit_transform(X[['key']]).toarray()\n",
    "columns_key = ['key_'+str(i) for i in list(set(X['key'].values))[1:]]\n",
    "features = pd.DataFrame(feature_arr, columns = columns_key, index = X.index)\n",
    "X = pd.concat([X, features], axis=1).drop(['key'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [\n",
    "    # {\n",
    "    #     'max_depth': np.arange(5, 15),\n",
    "    # },\n",
    "    {\n",
    "        'max_depth': np.arange(1, 20),\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "    },\n",
    "    {\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "        'max_leaf_nodes': np.arange(256, 324)\n",
    "    },\n",
    "    {\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "        'min_samples_split': np.arange(50, 700)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Available regression metrics are given here: https://scikit-learn.org/stable/modules/classes.html#regression-metric\n",
    "# https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions/42230764#42230764\n",
    "# ensure scikit is >0.18\n",
    "\n",
    "\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "print(\"Tuning hyper-parameters begin!\")\n",
    "print()\n",
    "\n",
    "# clf = GridSearchCV(tree.DecisionTreeRegressor(), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3)\n",
    "clf = HalvingGridSearchCV(tree.DecisionTreeRegressor(random_state=1), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3, random_state=1)\n",
    "print(\"Classifiers established, training data\")\n",
    "print()\n",
    "\n",
    "clf.fit(X, y)\n",
    "non_nested_scores = clf.best_score_\n",
    "print(\"Best parameters found:\", clf.best_params_)\n",
    "print(\"Score (mean squared):\", -clf.best_score_)\n",
    "\n",
    "print(\"Running cross validation\")\n",
    "print()\n",
    "clf.best_params_[\"random_state\"] = 1\n",
    "# cross_val_raw_data = cross_validate(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3, return_estimator=True, return_train_score=True)\n",
    "clf = tree.DecisionTreeRegressor(**clf.best_params_)\n",
    "cv_score = cross_val_score(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3,  scoring='neg_mean_squared_error')\n",
    "print(\"Cross validation score (mean squared):\", -cv_score.mean())\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('Feature importances:')\n",
    "for i, col in enumerate(X.columns):\n",
    "    print(f'{col:18}: {clf.feature_importances_[i]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report grid search results\n",
    "kwargs = {'ccp_alpha': 0.0003, 'criterion': 'mse', 'max_depth': 9, 'max_leaf_nodes': 260}\n",
    "clf = tree.DecisionTreeRegressor(**kwargs)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"r2: {clf.score(X_test, y_test)}\")\n",
    "print(f\"rmse: {mean_squared_error(y_test, clf.predict(X_test), squared=False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = [\"mse\", \"friedman_mse\"] # didn't make a big difference\n",
    "max_depth = 9 # found to result in best accuracy TODO: test over a range\n",
    "\n",
    "clf = tree.DecisionTreeRegressor(criterion=\"mse\", max_depth=max_depth)\n",
    "#     clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "# print(cross_val_score(clf, X_train, y_train, cv=cv))\n",
    "\n",
    "\n",
    "# Following a tutorial on Cost Complexity Pruning https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating different classifiers having different ccp_alpha values\n",
    "# WARNING: this takes a very long time to run! Below, ccp_alphas is pruned to cut down on computation time.\n",
    "# clfs = []\n",
    "# NUM_CCP_ALPHAS = 5\n",
    "# ccp_alphas = ccp_alphas[:NUM_CCP_ALPHAS]\n",
    "# for ccp_alpha in ccp_alphas:   \n",
    "#     print(f\"ccp_alpha: {ccp_alpha}\")\n",
    "#     clf = tree.DecisionTreeRegressor(random_state=0, ccp_alpha=ccp_alpha)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     clfs.append(clf)\n",
    "# print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "#       clfs[-1].tree_.node_count, ccp_alphas[-1]))\n",
    "\n",
    "# search for optimal ccp_alpha (seems to be somewhere in range 10**-4 to 10**-6)\n",
    "# 0 (99%/-0.6%), 0.0005 (34.3%/33.4%), 0.0000005 (88.3%/5.49%)\n",
    "for x in range(2,7):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, ccp_alpha=10**-x)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"alpha: {10**-x}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal tree depth (9)\n",
    "# TODO: plot different depths to demonstrate overfitting as depth increases past 9\n",
    "for md in range(2,25):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_depth=md)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"max_depth: {md}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal max leaf nodes value (maxima somewhere in (256, 324))\n",
    "for ln in range(2,25):\n",
    "    max_leaf_nodes = ln**2\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_leaf_nodes=max_leaf_nodes)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"max_leaf_node: {max_leaf_nodes}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal min no. leaf samples\n",
    "for msl in range(1,100,5):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_leaf_nodes=298, min_samples_leaf=msl)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"min_samples_leaf: {msl}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "# test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "# print(train_scores)\n",
    "# print(test_scores)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# ax.set_xlabel(\"alpha\")\n",
    "# ax.set_ylabel(\"accuracy\")\n",
    "# ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "# ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "#         drawstyle=\"steps-post\")\n",
    "# ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "#         drawstyle=\"steps-post\")\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perform nested cross-validation (https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw tree\n",
    "# clf.get_depth()\n",
    "# plt.figure(figsize=(50,12))\n",
    "# tree.plot_tree(clf,  fontsize=10, feature_names=headers)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyper-parameters begin!\n",
      "\n",
      "Classifiers established, training data\n",
      "\n",
      "n_iterations: 9\n",
      "n_required_iterations: 10\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 10\n",
      "max_resources_: 132427\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 37587\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 37587 candidates, totalling 187935 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 12529\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 12529 candidates, totalling 62645 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 4177\n",
      "n_resources: 90\n",
      "Fitting 5 folds for each of 4177 candidates, totalling 20885 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 1393\n",
      "n_resources: 270\n",
      "Fitting 5 folds for each of 1393 candidates, totalling 6965 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 465\n",
      "n_resources: 810\n",
      "Fitting 5 folds for each of 465 candidates, totalling 2325 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 155\n",
      "n_resources: 2430\n",
      "Fitting 5 folds for each of 155 candidates, totalling 775 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 52\n",
      "n_resources: 7290\n",
      "Fitting 5 folds for each of 52 candidates, totalling 260 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 18\n",
      "n_resources: 21870\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 6\n",
      "n_resources: 65610\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters found: {'ccp_alpha': 3.938775510204081e-05, 'min_samples_split': 238}\n",
      "Score (mean squared): 0.02545851270621157\n",
      "Running cross validation\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:    2.3s remaining:    3.5s\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:    3.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score (mean squared): 0.025437486988483925\n",
      "Tuning hyper-parameters begin!\n",
      "\n",
      "Classifiers established, training data\n",
      "\n",
      "n_iterations: 9\n",
      "n_required_iterations: 10\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 10\n",
      "max_resources_: 132427\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 37587\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 37587 candidates, totalling 187935 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9621038e8d18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0mnon_nested_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameters found:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search_successive_halving.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_samples_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search_successive_halving.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m    317\u001b[0m                             'n_resources': [n_resources] * n_candidates}\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m             results = evaluate_candidates(candidate_params, cv,\n\u001b[0m\u001b[1;32m    320\u001b[0m                                           more_results=more_results)\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# p value testing\n",
    "num_tests = 10\n",
    "tests_scores = []\n",
    "\n",
    "tuned_parameters = [\n",
    "    # {\n",
    "    #     'max_depth': np.arange(5, 15),\n",
    "    # },\n",
    "    {\n",
    "        'max_depth': np.arange(1, 20),\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "    },\n",
    "    {\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "        'max_leaf_nodes': np.arange(256, 324)\n",
    "    },\n",
    "    {\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "        'min_samples_split': np.arange(50, 700)\n",
    "    }\n",
    "]\n",
    "\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "print(\"Tuning hyper-parameters begin!\")\n",
    "print()\n",
    "\n",
    "# clf = GridSearchCV(tree.DecisionTreeRegressor(), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3)\n",
    "clf = HalvingGridSearchCV(tree.DecisionTreeRegressor(random_state=1), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3, random_state=1)\n",
    "print(\"Classifiers established, training data\")\n",
    "print()\n",
    "\n",
    "clf.fit(X, y)\n",
    "non_nested_scores = clf.best_score_\n",
    "print(\"Best parameters found:\", clf.best_params_)\n",
    "print(\"Score (mean squared):\", -clf.best_score_)\n",
    "\n",
    "print(\"Running cross validation\")\n",
    "print()\n",
    "clf.best_params_[\"random_state\"] = 1\n",
    "# cross_val_raw_data = cross_validate(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3, return_estimator=True, return_train_score=True)\n",
    "clf = tree.DecisionTreeRegressor(**clf.best_params_)\n",
    "cv_score = cross_val_score(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3,  scoring='neg_mean_squared_error')\n",
    "print(\"Cross validation score (mean squared):\", -cv_score.mean())\n",
    "orig_score = -cv_score.mean()\n",
    "\n",
    "for i in range(num_tests):\n",
    "    np.random.shuffle(y)\n",
    "    tuned_parameters = [\n",
    "        # {\n",
    "        #     'max_depth': np.arange(5, 15),\n",
    "        # },\n",
    "        {\n",
    "            'max_depth': np.arange(1, 20),\n",
    "            'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "        },\n",
    "        {\n",
    "            'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "            'max_leaf_nodes': np.arange(256, 324)\n",
    "        },\n",
    "        {\n",
    "            'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "            'min_samples_split': np.arange(50, 700)\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "    outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    print(\"Tuning hyper-parameters begin!\")\n",
    "    print()\n",
    "\n",
    "    # clf = GridSearchCV(tree.DecisionTreeRegressor(), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3)\n",
    "    clf = HalvingGridSearchCV(tree.DecisionTreeRegressor(random_state=1), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3, random_state=1)\n",
    "    print(\"Classifiers established, training data\")\n",
    "    print()\n",
    "\n",
    "    clf.fit(X, y)\n",
    "    non_nested_scores = clf.best_score_\n",
    "    print(\"Best parameters found:\", clf.best_params_)\n",
    "    print(\"Score (mean squared):\", -clf.best_score_)\n",
    "\n",
    "    print(\"Running cross validation\")\n",
    "    print()\n",
    "    clf.best_params_[\"random_state\"] = 1\n",
    "    # cross_val_raw_data = cross_validate(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3, return_estimator=True, return_train_score=True)\n",
    "    clf = tree.DecisionTreeRegressor(**clf.best_params_)\n",
    "    cv_score = cross_val_score(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3,  scoring='neg_mean_squared_error')\n",
    "    print(\"Cross validation score (mean squared):\", -cv_score.mean())\n",
    "    tests_scores.append(-cv_score.mean())\n",
    "    \n",
    "tset, pval = ttest_1samp(tests_scores, orig_score)\n",
    "print(\"p-value\", pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
