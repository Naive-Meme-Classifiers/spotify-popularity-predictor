{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install directly into notebook\n",
    "%pip install sklearn\n",
    "%pip install pandas\n",
    "%pip install imbalanced-learn\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install scipy\n",
    "%pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, KFold, cross_validate\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import time\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-radius",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warning for chained assignment (not necessary but cleans up the project)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# taken from kaggle example\n",
    "class ReplaceZeroTransformer():\n",
    "    \"\"\"Eliminates Zero values from tempo columns and replace it \n",
    "       with the median or mean of non-zero values as specified.\n",
    "       defaut is set to 'median'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='median'):\n",
    "        self.method = method\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.method == 'median':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].median()\n",
    "        elif self.method == 'mean':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].mean()\n",
    "        else:\n",
    "            raise Exception(\"Method can be 'median' or 'mean' only!\")\n",
    "        return X\n",
    "    \n",
    "# Import track data\n",
    "usecols = ['acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode','popularity', 'speechiness', 'tempo', 'valence']\n",
    "dataset = pd.read_csv(\"data.csv\", header = 0, usecols=usecols)\n",
    "\n",
    "# Remove rows duplicated by ignoring some columns\n",
    "dataset = dataset[~dataset.duplicated()==1]\n",
    "\n",
    "# Normalize columns having values outside [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "# cols_to_normalize = ['duration_ms', 'key', 'loudness', 'popularity', 'tempo']\n",
    "cols_to_normalize = ['duration_ms', 'loudness', 'tempo']\n",
    "dataset[cols_to_normalize] = scaler.fit_transform(dataset[cols_to_normalize])\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "# TODO: further preprocessing?\n",
    "\n",
    "y = dataset.pop('popularity') # popularity is our class to predict\n",
    "X_headers = list(dataset.columns.values)\n",
    "X = dataset\n",
    "\n",
    "# Create the under sampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "# apply the transform\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "tempo_transformer = ReplaceZeroTransformer()\n",
    "X = tempo_transformer.transform(X)\n",
    "\n",
    "# need to scale after to treat the individual categories as their own class for the undersampling\n",
    "y = y/100\n",
    "\n",
    "# one hot encode the keys since they are a multiclass\n",
    "ohe = OneHotEncoder(categories='auto', drop='first')\n",
    "\n",
    "feature_arr = ohe.fit_transform(X[['key']]).toarray()\n",
    "columns_key = ['key_'+str(i) for i in list(set(X['key'].values))[1:]]\n",
    "features = pd.DataFrame(feature_arr, columns = columns_key, index = X.index)\n",
    "X = pd.concat([X, features], axis=1).drop(['key'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-emperor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-layer perception: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "regr = MLPRegressor(hidden_layer_sizes=(100, 10, 5, 1), random_state=1, max_iter=500).fit(X_train, y_train)\n",
    "\n",
    "# hidden_layer_sizes \n",
    "# ------ activation: identity, logistic, tanh, relu\n",
    "# ------ solver: lbfgs, sgd, adam\n",
    "# alpha (L2 penalty)\n",
    "# learning_rate_init\n",
    "\n",
    "print(f\"num layers: {regr.n_layers_}\")\n",
    "# print(f\"R^2: {regr.score(X_test, y_test)}\")\n",
    "# print(f\"rmse: {mean_squared_error(y_test, regr.predict(X_test), squared=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "immediate-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp checking enum parameters\n",
    "activations = ['relu'] \n",
    "solver = ['adam']\n",
    "\n",
    "r = MLPRegressor(random_state=1)\n",
    "r.fit(X_train, y_train)\n",
    "print(f\"rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n",
    "        \n",
    "# best performing = relu w/ lbfgs => takes a really long time to train\n",
    "# focusing on relu/adam; relu performed the best, although lbfgs is better,\n",
    "#  it takes too many iterations to be valuable, it would take too long to be feasible\n",
    "# the docs also specify adam is better performing on relatively large datasets compared\n",
    "#  lbfgs which is said to converge faster and perform better on small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = MLPRegressor(random_state=1, activation='relu', solver='lbfgs', max_iter=500)\n",
    "r.fit(X_train, y_train)\n",
    "print(f\"rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 hidden layers best\n",
    "# layers = [(50), (50, 50), (50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50, 50)]\n",
    "# for l in layers:\n",
    "#     r = MLPRegressor(random_state=1, hidden_layer_sizes=l)\n",
    "#     r.fit(X_train, y_train)\n",
    "#     print(f\"l={l}, rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n",
    "    \n",
    "# layer_size = [25, 50, 100, 200]\n",
    "# num_layers = [1, 2, 3, 4, 5]\n",
    "# for i in layer_size:\n",
    "#     l = list()\n",
    "#     for j in num_layers:\n",
    "#         for k in range(j):\n",
    "#             l.append(i)\n",
    "#         t = tuple(l)\n",
    "#         r = MLPRegressor(random_state=1, hidden_layer_sizes=t)\n",
    "#         r.fit(X_train, y_train)\n",
    "#         print(f\"layer_size={i}, num_layers={j}, rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n",
    "\n",
    "# 25:  4 = 0.15546668386556156\n",
    "# 50:  3 = 0.15460937803611588\n",
    "# 100: 4 = 0.15271251400531363\n",
    "# 200: 3 = 0.1514766594636612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [\n",
    "    {\n",
    "        # 'alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "        'alpha': 10.0 **-np.arange(1, 7)\n",
    "        # 'learning_rate_init': np.append(0, np.linspace(0.0001, 0.01, 50))\n",
    "    }\n",
    "]\n",
    "\n",
    "# Available regression metrics are given here: https://scikit-learn.org/stable/modules/classes.html#regression-metric\n",
    "# https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions/42230764#42230764\n",
    "# ensure scikit is >0.18\n",
    "\n",
    "\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "print(\"Tuning hyper-parameters begin!\")\n",
    "print()\n",
    "\n",
    "# clf = GridSearchCV(tree.DecisionTreeRegressor(), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3)\n",
    "clf = HalvingGridSearchCV(MLPRegressor(random_state=1), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3, random_state=1)\n",
    "print(\"Classifiers established, training data\")\n",
    "print()\n",
    "\n",
    "clf.fit(X, y)\n",
    "non_nested_scores = clf.best_score_\n",
    "print(\"Best parameters found:\", clf.best_params_)\n",
    "print(\"Score (mean squared):\", -clf.best_score_)\n",
    "\n",
    "print(\"Running cross validation\")\n",
    "print()\n",
    "clf.best_params_[\"random_state\"] = 1\n",
    "# cross_val_raw_data = cross_validate(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3, return_estimator=True, return_train_score=True)\n",
    "# clf = tree.DecisionTreeRegressor(**clf.best_params_)\n",
    "clf = MLPRegressor(**clf.best_params_)\n",
    "cv_score = cross_val_score(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3,  scoring='neg_mean_squared_error')\n",
    "print(\"Cross validation score (mean squared):\", -cv_score.mean())\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current best model\n",
    "model = MLPRegressor(hidden_layer_sizes=(200, 200, 200))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.loss_curve_).plot(xlabel='Epoch', ylabel='Loss', title='Loss vs Epoch', legend=False, xticks=[0, 5, 10, 15, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-highway",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(200, 200, 200))\n",
    "model.fit(X_train, y_train)\n",
    "orig_score = mean_squared_error(y_test, model.predict(X_test), squared=False)\n",
    "\n",
    "num_tests = 30\n",
    "test_scores = []\n",
    "\n",
    "for i in range(num_tests):\n",
    "    np.random.shuffle(y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    model = MLPRegressor(hidden_layer_sizes=(200, 200, 200))\n",
    "    model.fit(X_train, y_train)\n",
    "    rmse = mean_squared_error(y_test, model.predict(X_test), squared=False)\n",
    "    test_scores.append(rmse)\n",
    "    \n",
    "tset, pval = ttest_1samp(test_scores, orig_score)\n",
    "print(\"p-value\", pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orig_score)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegressor(hidden_layer_sizes=(200, 200, 200))\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-martin",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "# use Kernel SHAP to explain test set predictions\n",
    "sample_train = shap.utils.sample(X_train, nsamples=100, random_state=0)\n",
    "explainer = shap.KernelExplainer(model.predict, sample_train, link=\"identity\")\n",
    "\n",
    "shap_values = explainer.shap_values(X_test, nsamples=100)\n",
    "\n",
    "# plot the SHAP values for the Setosa output of the first instance\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=\"identity\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
