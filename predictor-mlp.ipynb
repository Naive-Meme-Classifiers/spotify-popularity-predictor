{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /home/robert/anaconda3/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/robert/anaconda3/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/robert/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.5.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /home/robert/anaconda3/lib/python3.8/site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/robert/.local/lib/python3.8/site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/robert/.local/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/robert/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/robert/.local/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imbalanced-learn in /home/robert/anaconda3/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/robert/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/robert/.local/lib/python3.8/site-packages (from imbalanced-learn) (1.19.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/robert/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/robert/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /home/robert/anaconda3/lib/python3.8/site-packages (3.2.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/robert/anaconda3/lib/python3.8/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/robert/.local/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/robert/anaconda3/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /home/robert/.local/lib/python3.8/site-packages (from matplotlib) (1.19.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/robert/anaconda3/lib/python3.8/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/robert/.local/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /home/robert/.local/lib/python3.8/site-packages (1.19.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scipy in /home/robert/anaconda3/lib/python3.8/site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/robert/.local/lib/python3.8/site-packages (from scipy) (1.19.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install directly into notebook\n",
    "%pip install sklearn\n",
    "%pip install pandas\n",
    "%pip install imbalanced-learn\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, KFold, cross_validate\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import time\n",
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warning for chained assignment (not necessary but cleans up the project)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# taken from kaggle example\n",
    "class ReplaceZeroTransformer():\n",
    "    \"\"\"Eliminates Zero values from tempo columns and replace it \n",
    "       with the median or mean of non-zero values as specified.\n",
    "       defaut is set to 'median'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='median'):\n",
    "        self.method = method\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.method == 'median':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].median()\n",
    "        elif self.method == 'mean':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].mean()\n",
    "        else:\n",
    "            raise Exception(\"Method can be 'median' or 'mean' only!\")\n",
    "        return X\n",
    "    \n",
    "# Import track data\n",
    "usecols = ['acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode','popularity', 'speechiness', 'tempo', 'valence']\n",
    "dataset = pd.read_csv(\"data.csv\", header = 0, usecols=usecols)\n",
    "\n",
    "# Remove rows duplicated by ignoring some columns\n",
    "dataset = dataset[~dataset.duplicated()==1]\n",
    "\n",
    "# Normalize columns having values outside [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "# cols_to_normalize = ['duration_ms', 'key', 'loudness', 'popularity', 'tempo']\n",
    "cols_to_normalize = ['duration_ms', 'loudness', 'tempo']\n",
    "dataset[cols_to_normalize] = scaler.fit_transform(dataset[cols_to_normalize])\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "# TODO: further preprocessing?\n",
    "\n",
    "y = dataset.pop('popularity') # popularity is our class to predict\n",
    "X_headers = list(dataset.columns.values)\n",
    "X = dataset\n",
    "\n",
    "# Create the under sampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "# apply the transform\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "tempo_transformer = ReplaceZeroTransformer()\n",
    "X = tempo_transformer.transform(X)\n",
    "\n",
    "# need to scale after to treat the individual categories as their own class for the undersampling\n",
    "y = y/100\n",
    "\n",
    "# one hot encode the keys since they are a multiclass\n",
    "ohe = OneHotEncoder(categories='auto', drop='first')\n",
    "\n",
    "feature_arr = ohe.fit_transform(X[['key']]).toarray()\n",
    "columns_key = ['key_'+str(i) for i in list(set(X['key'].values))[1:]]\n",
    "features = pd.DataFrame(feature_arr, columns = columns_key, index = X.index)\n",
    "X = pd.concat([X, features], axis=1).drop(['key'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-layer perception: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "regr = MLPRegressor(hidden_layer_sizes=(100, 10, 5, 1), random_state=1, max_iter=500).fit(X_train, y_train)\n",
    "\n",
    "# hidden_layer_sizes \n",
    "# ------ activation: identity, logistic, tanh, relu\n",
    "# ------ solver: lbfgs, sgd, adam\n",
    "# alpha (L2 penalty)\n",
    "# learning_rate_init\n",
    "\n",
    "print(f\"num layers: {regr.n_layers_}\")\n",
    "# print(f\"R^2: {regr.score(X_test, y_test)}\")\n",
    "# print(f\"rmse: {mean_squared_error(y_test, regr.predict(X_test), squared=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp checking enum parameters\n",
    "activations = ['relu'] \n",
    "solver = ['adam']\n",
    "\n",
    "r = MLPRegressor(random_state=1)\n",
    "r.fit(X_train, y_train)\n",
    "print(f\"rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n",
    "        \n",
    "# best performing = relu w/ lbfgs => takes a really long time to train\n",
    "# focusing on relu/adam; relu performed the best, although lbfgs is better,\n",
    "#  it takes too many iterations to be valuable, it would take too long to be feasible\n",
    "# the docs also specify adam is better performing on relatively large datasets compared\n",
    "#  lbfgs which is said to converge faster and perform better on small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = MLPRegressor(random_state=1, activation='relu', solver='lbfgs', max_iter=500)\n",
    "r.fit(X_train, y_train)\n",
    "print(f\"rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 hidden layers best\n",
    "# layers = [(50), (50, 50), (50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50, 50)]\n",
    "# for l in layers:\n",
    "#     r = MLPRegressor(random_state=1, hidden_layer_sizes=l)\n",
    "#     r.fit(X_train, y_train)\n",
    "#     print(f\"l={l}, rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n",
    "    \n",
    "layer_size = [25, 50, 100, 200]\n",
    "num_layers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "for i in layer_size:\n",
    "    for j in num_layers:\n",
    "        l = list()\n",
    "        for k in range(j):\n",
    "            l.append(i)\n",
    "        t = tuple(l)\n",
    "        r = MLPRegressor(random_state=1, hidden_layer_sizes=t)\n",
    "        r.fit(X_train, y_train)\n",
    "        print(f\"layer_size={i}, num_layers={j}, rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n",
    "\n",
    "# 25:  4 = 0.15546668386556156\n",
    "# 50:  3 = 0.15460937803611588\n",
    "# 100: 4 = 0.15271251400531363\n",
    "# 200: 3 = 0.1514766594636612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "layer_size = [25, 50, 100, 200]\n",
    "num_layers = [1, 2, 3, 4, 5]\n",
    "nodes = []\n",
    "for i in layer_size:\n",
    "    for j in num_layers:\n",
    "        l = []\n",
    "        for k in range(j):\n",
    "            l.append(i)\n",
    "        t = tuple(l)\n",
    "        nodes.append(t)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [\n",
    "    {\n",
    "        'alpha': 10.0 **-np.arange(1, 7)\n",
    "        # 'learning_rate_init': np.append(0, np.linspace(0.0001, 0.01, 50))\n",
    "    }\n",
    "]\n",
    "\n",
    "# Available regression metrics are given here: https://scikit-learn.org/stable/modules/classes.html#regression-metric\n",
    "# https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions/42230764#42230764\n",
    "# ensure scikit is >0.18\n",
    "\n",
    "\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "print(\"Tuning hyper-parameters begin!\")\n",
    "print()\n",
    "\n",
    "# clf = GridSearchCV(tree.DecisionTreeRegressor(), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3)\n",
    "clf = HalvingGridSearchCV(MLPRegressor(random_state=1), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3, random_state=1)\n",
    "print(\"Classifiers established, training data\")\n",
    "print()\n",
    "\n",
    "clf.fit(X, y)\n",
    "non_nested_scores = clf.best_score_\n",
    "print(\"Best parameters found:\", clf.best_params_)\n",
    "print(\"Score (mean squared):\", -clf.best_score_)\n",
    "\n",
    "print(\"Running cross validation\")\n",
    "print()\n",
    "clf.best_params_[\"random_state\"] = 1\n",
    "# cross_val_raw_data = cross_validate(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3, return_estimator=True, return_train_score=True)\n",
    "# clf = tree.DecisionTreeRegressor(**clf.best_params_)\n",
    "clf = MLPRegressor(**clf.best_params_)\n",
    "cv_score = cross_val_score(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3,  scoring='neg_mean_squared_error')\n",
    "print(\"Cross validation score (mean squared):\", -cv_score.mean())\n",
    "\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current best model\n",
    "model = MLPRegressor(hidden_layer_sizes=(200, 200, 200))\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(model.loss_curve_).plot(xlabel='Epoch', ylabel='Loss', title='Loss vs Epoch', legend=False, xticks=[0, 5, 10, 15, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:  2.7min remaining:  4.1min\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse:  0.02300733195711797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:  2.4min remaining:  3.5min\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  4.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03605610076655559\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:  2.1min remaining:  3.2min\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  4.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036027652017335744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:  1.8min remaining:  2.7min\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036018758760148525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:  1.8min remaining:  2.7min\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  3.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035966727697019946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:  1.8min remaining:  2.7min\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0360237100298382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:  1.9min remaining:  2.9min\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03596087950065056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:  1.9min remaining:  2.9min\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03597800064731154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:  1.9min remaining:  2.9min\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.035969140543394604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:  1.9min remaining:  2.9min\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  3.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03610336813159012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:  1.9min remaining:  2.9min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.036011359199682694\n",
      "p-value 1.1885953452392224e-23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:  3.5min finished\n"
     ]
    }
   ],
   "source": [
    "orig_scores = []\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "model = MLPRegressor(hidden_layer_sizes=(200, 200, 200, 200, 200))\n",
    "cv_scores = cross_val_score(model, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3,  scoring='neg_mean_squared_error')\n",
    "orig_score = -cv_scores.mean()\n",
    "print(\"rmse: \", orig_score)\n",
    "num_tests = 10\n",
    "test_scores = []\n",
    "\n",
    "for i in range(num_tests):\n",
    "    np.random.shuffle(y)\n",
    "    model = MLPRegressor(hidden_layer_sizes=(200, 200, 200, 200, 200))\n",
    "    cv_scores = cross_val_score(model, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3,  scoring='neg_mean_squared_error')\n",
    "    rmse = -cv_scores.mean()\n",
    "    print(rmse)\n",
    "    test_scores.append(rmse)\n",
    "    \n",
    "tset, pval = ttest_1samp(test_scores, orig_score)\n",
    "print(\"p-value\", pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02300733195711797\n",
      "[0.03605610076655559, 0.036027652017335744, 0.036018758760148525, 0.035966727697019946, 0.0360237100298382, 0.03596087950065056, 0.03597800064731154, 0.035969140543394604, 0.03610336813159012, 0.036011359199682694]\n"
     ]
    }
   ],
   "source": [
    "print(orig_score)\n",
    "print(test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE = 0.1516816797\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
