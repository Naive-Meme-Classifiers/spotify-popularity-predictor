{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "immediate-diamond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sklearn in /home/shadow/.local/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/shadow/.local/lib/python3.9/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/shadow/.local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/shadow/.local/lib/python3.9/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/shadow/.local/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/lib64/python3.9/site-packages (from scikit-learn->sklearn) (1.19.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/shadow/.local/lib/python3.9/site-packages (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/lib/python3.9/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3.9/site-packages (from pandas) (2020.5)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /usr/lib64/python3.9/site-packages (from pandas) (1.19.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in /home/shadow/.local/lib/python3.9/site-packages (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/lib64/python3.9/site-packages (from imbalanced-learn) (1.19.4)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/shadow/.local/lib/python3.9/site-packages (from imbalanced-learn) (1.6.0)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/shadow/.local/lib/python3.9/site-packages (from imbalanced-learn) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/shadow/.local/lib/python3.9/site-packages (from imbalanced-learn) (1.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/shadow/.local/lib/python3.9/site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install directly into notebook\n",
    "%pip install sklearn\n",
    "%pip install pandas\n",
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "piano-style",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, KFold, cross_validate\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "three-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warning for chained assignment (not necessary but cleans up the project)\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# taken from kaggle example\n",
    "class ReplaceZeroTransformer():\n",
    "    \"\"\"Eliminates Zero values from tempo columns and replace it \n",
    "       with the median or mean of non-zero values as specified.\n",
    "       defaut is set to 'median'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='median'):\n",
    "        self.method = method\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.method == 'median':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].median()\n",
    "        elif self.method == 'mean':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].mean()\n",
    "        else:\n",
    "            raise Exception(\"Method can be 'median' or 'mean' only!\")\n",
    "        return X\n",
    "    \n",
    "# Import track data\n",
    "usecols = ['acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode','popularity', 'speechiness', 'tempo', 'valence']\n",
    "dataset = pd.read_csv(\"data.csv\", header = 0, usecols=usecols)\n",
    "\n",
    "# Remove rows duplicated by ignoring some columns\n",
    "dataset = dataset[~dataset.duplicated()==1]\n",
    "\n",
    "# Normalize columns having values outside [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "# cols_to_normalize = ['duration_ms', 'key', 'loudness', 'popularity', 'tempo']\n",
    "cols_to_normalize = ['duration_ms', 'loudness', 'tempo']\n",
    "dataset[cols_to_normalize] = scaler.fit_transform(dataset[cols_to_normalize])\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "# TODO: further preprocessing?\n",
    "\n",
    "y = dataset.pop('popularity') # popularity is our class to predict\n",
    "X_headers = list(dataset.columns.values)\n",
    "X = dataset\n",
    "\n",
    "# Create the under sampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "# apply the transform\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "tempo_transformer = ReplaceZeroTransformer()\n",
    "X = tempo_transformer.transform(X)\n",
    "\n",
    "# need to scale after to treat the individual categories as their own class for the undersampling\n",
    "y = y/100\n",
    "\n",
    "# one hot encode the keys since they are a multiclass\n",
    "ohe = OneHotEncoder(categories='auto', drop='first')\n",
    "\n",
    "feature_arr = ohe.fit_transform(X[['key']]).toarray()\n",
    "columns_key = ['key_'+str(i) for i in list(set(X['key'].values))[1:]]\n",
    "features = pd.DataFrame(feature_arr, columns = columns_key, index = X.index)\n",
    "X = pd.concat([X, features], axis=1).drop(['key'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "complex-absorption",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num layers: 6\n"
     ]
    }
   ],
   "source": [
    "# multi-layer perception: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "regr = MLPRegressor(hidden_layer_sizes=(100, 10, 5, 1), random_state=1, max_iter=500).fit(X_train, y_train)\n",
    "\n",
    "# hidden_layer_sizes \n",
    "# ------ activation: identity, logistic, tanh, relu\n",
    "# ------ solver: lbfgs, sgd, adam\n",
    "# alpha (L2 penalty)\n",
    "# learning_rate_init\n",
    "\n",
    "print(f\"num layers: {regr.n_layers_}\")\n",
    "# print(f\"R^2: {regr.score(X_test, y_test)}\")\n",
    "# print(f\"rmse: {mean_squared_error(y_test, regr.predict(X_test), squared=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "engaged-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shadow/.local/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:619: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r=MLPRegressor(random_state=1), rmse: 0.16047461662256637\n",
      "r=MLPRegressor(learning_rate='invscaling', random_state=1), rmse: 0.1569349994123843\n",
      "r=MLPRegressor(learning_rate='adaptive', random_state=1), rmse: 0.1569349994123843\n"
     ]
    }
   ],
   "source": [
    "# mlp checking enum parameters\n",
    "activations = ['relu'] \n",
    "solver = ['adam']\n",
    "\n",
    "r = MLPRegressor(random_state=1)\n",
    "r.fit(X_train, y_train)\n",
    "print(f\"rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n",
    "        \n",
    "# best performing = relu w/ lbfgs => takes a really long time to train\n",
    "# focusing on relu/adam; relu performed the best, although lbfgs is better,\n",
    "#  it takes too many iterations to be valuable, it would take too long to be feasible\n",
    "# the docs also specify adam is better performing on relatively large datasets compared\n",
    "#  lbfgs which is said to converge faster and perform better on small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "above-senator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse: 0.15466535492409164\n"
     ]
    }
   ],
   "source": [
    "r = MLPRegressor(random_state=1, activation='relu', solver='lbfgs', max_iter=500)\n",
    "r.fit(X_train, y_train)\n",
    "print(f\"rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "weighted-switch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_size=25, num_layers=1, rmse: 0.1598068542465586\n",
      "layer_size=25, num_layers=2, rmse: 0.15711373627716135\n",
      "layer_size=25, num_layers=3, rmse: 0.15578702118541113\n",
      "layer_size=25, num_layers=4, rmse: 0.15546668386556156\n",
      "layer_size=25, num_layers=5, rmse: 0.15853275412943318\n",
      "layer_size=50, num_layers=1, rmse: 0.15755816180351992\n",
      "layer_size=50, num_layers=2, rmse: 0.15513733148840209\n",
      "layer_size=50, num_layers=3, rmse: 0.15460937803611588\n",
      "layer_size=50, num_layers=4, rmse: 0.1549226627128356\n",
      "layer_size=50, num_layers=5, rmse: 0.15468368273642918\n",
      "layer_size=100, num_layers=1, rmse: 0.1569349994123843\n",
      "layer_size=100, num_layers=2, rmse: 0.1543123573361768\n",
      "layer_size=100, num_layers=3, rmse: 0.15535083253801144\n",
      "layer_size=100, num_layers=4, rmse: 0.15271251400531363\n",
      "layer_size=100, num_layers=5, rmse: 0.15436590013317678\n",
      "layer_size=200, num_layers=1, rmse: 0.15620503142642106\n",
      "layer_size=200, num_layers=2, rmse: 0.15178929095979837\n",
      "layer_size=200, num_layers=3, rmse: 0.1514766594636612\n",
      "layer_size=200, num_layers=4, rmse: 0.15342885272852955\n",
      "layer_size=200, num_layers=5, rmse: 0.1530739812690993\n"
     ]
    }
   ],
   "source": [
    "# 4 hidden layers best\n",
    "# layers = [(50), (50, 50), (50, 50, 50), (50, 50, 50, 50), (50, 50, 50, 50, 50)]\n",
    "# for l in layers:\n",
    "#     r = MLPRegressor(random_state=1, hidden_layer_sizes=l)\n",
    "#     r.fit(X_train, y_train)\n",
    "#     print(f\"l={l}, rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n",
    "    \n",
    "layer_size = [25, 50, 100, 200]\n",
    "num_layers = [1, 2, 3, 4, 5]\n",
    "for i in layer_size:\n",
    "    l = list()\n",
    "    for j in num_layers:\n",
    "        for k in range(j):\n",
    "            l.append(i)\n",
    "        t = tuple(l)\n",
    "        r = MLPRegressor(random_state=1, hidden_layer_sizes=t)\n",
    "        r.fit(X_train, y_train)\n",
    "        print(f\"layer_size={i}, num_layers={j}, rmse: {mean_squared_error(y_test, r.predict(X_test), squared=False)}\")\n",
    "\n",
    "# 25:  4 = 0.15546668386556156\n",
    "# 50:  3 = 0.15460937803611588\n",
    "# 100: 4 = 0.15271251400531363\n",
    "# 200: 3 = 0.1514766594636612"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-monroe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [\n",
    "    # {\n",
    "    #     'hidden_layer_sizes': np.arange(5, 15),\n",
    "    # },\n",
    "    {\n",
    "        'alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "        'learning_rate_init': np.append(0, np.linspace(0.000001, 0.0001, 50))\n",
    "    }\n",
    "]\n",
    "\n",
    "# Available regression metrics are given here: https://scikit-learn.org/stable/modules/classes.html#regression-metric\n",
    "# https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions/42230764#42230764\n",
    "# ensure scikit is >0.18\n",
    "\n",
    "\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "print(\"Tuning hyper-parameters begin!\")\n",
    "print()\n",
    "\n",
    "# clf = GridSearchCV(tree.DecisionTreeRegressor(), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3)\n",
    "clf = HalvingGridSearchCV(MLPRegressor(random_state=1), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3, random_state=1)\n",
    "print(\"Classifiers established, training data\")\n",
    "print()\n",
    "\n",
    "clf.fit(X, y)\n",
    "non_nested_scores = clf.best_score_\n",
    "print(\"Best parameters found:\", clf.best_params_)\n",
    "print(\"Score (mean squared):\", -clf.best_score_)\n",
    "\n",
    "print(\"Running cross validation\")\n",
    "print()\n",
    "clf.best_params_[\"random_state\"] = 1\n",
    "# cross_val_raw_data = cross_validate(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3, return_estimator=True, return_train_score=True)\n",
    "# clf = tree.DecisionTreeRegressor(**clf.best_params_)\n",
    "clf = MLPRegressor(**clf.best_params_)\n",
    "cv_score = cross_val_score(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3,  scoring='neg_mean_squared_error')\n",
    "print(\"Cross validation score (mean squared):\", -cv_score.mean())\n",
    "\n",
    "clf.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
