{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: sklearn in /home/capricorn-red/.local/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/capricorn-red/.local/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/capricorn-red/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/capricorn-red/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/capricorn-red/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/capricorn-red/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /home/capricorn-red/miniconda3/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/capricorn-red/.local/lib/python3.8/site-packages (from pandas) (2.7.5)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/capricorn-red/.local/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/capricorn-red/miniconda3/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/capricorn-red/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imbalanced-learn in /home/capricorn-red/miniconda3/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/capricorn-red/.local/lib/python3.8/site-packages (from imbalanced-learn) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/capricorn-red/.local/lib/python3.8/site-packages (from imbalanced-learn) (1.19.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/capricorn-red/.local/lib/python3.8/site-packages (from imbalanced-learn) (1.0.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/capricorn-red/.local/lib/python3.8/site-packages (from imbalanced-learn) (1.6.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/capricorn-red/.local/lib/python3.8/site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /home/capricorn-red/miniconda3/lib/python3.8/site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.15 in /home/capricorn-red/.local/lib/python3.8/site-packages (from seaborn) (1.19.5)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/capricorn-red/miniconda3/lib/python3.8/site-packages (from seaborn) (1.2.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/capricorn-red/.local/lib/python3.8/site-packages (from seaborn) (3.3.4)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/capricorn-red/.local/lib/python3.8/site-packages (from seaborn) (1.6.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/capricorn-red/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/capricorn-red/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (8.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/capricorn-red/.local/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/capricorn-red/.local/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.7.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/capricorn-red/.local/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.1)\n",
      "Requirement already satisfied: six in /home/capricorn-red/miniconda3/lib/python3.8/site-packages (from cycler>=0.10->matplotlib>=2.2->seaborn) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/capricorn-red/miniconda3/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install directly into notebook\n",
    "%pip install sklearn\n",
    "%pip install pandas\n",
    "%pip install imbalanced-learn\n",
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider making list of dependencies for TA to install when running this notebook\n",
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, KFold, cross_validate\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warning for chained assignment (not necessary but cleans up the project)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from kaggle example\n",
    "class ReplaceZeroTransformer():\n",
    "    \"\"\"Eliminates Zero values from tempo columns and replace it \n",
    "       with the median or mean of non-zero values as specified.\n",
    "       defaut is set to 'median'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='median'):\n",
    "        self.method = method\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.method == 'median':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].median()\n",
    "        elif self.method == 'mean':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].mean()\n",
    "        else:\n",
    "            raise Exception(\"Method can be 'median' or 'mean' only!\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import track data\n",
    "usecols = ['acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode','popularity', 'speechiness', 'tempo', 'valence']\n",
    "dataset = pd.read_csv(\"data.csv\", header = 0, usecols=usecols)\n",
    "\n",
    "# Remove rows duplicated by ignoring some columns\n",
    "dataset = dataset[~dataset.duplicated()==1]\n",
    "\n",
    "# Normalize columns having values outside [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "# cols_to_normalize = ['duration_ms', 'key', 'loudness', 'popularity', 'tempo']\n",
    "cols_to_normalize = ['duration_ms', 'loudness', 'tempo']\n",
    "dataset[cols_to_normalize] = scaler.fit_transform(dataset[cols_to_normalize])\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data analysis\n",
    "# pair plots: https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "import seaborn as sb\n",
    "\n",
    "# Pairwise plots between each column in the data set excluding popularity, binary (mode and explicit) and discrete (key) features\n",
    "pair_plot_data = dataset.drop(columns=['mode', 'explicit', 'key', 'popularity'])\n",
    "sb.pairplot(pair_plot_data, corner=True)\n",
    "# There isn't a lot of direct correlation between any two property of a song\n",
    "\n",
    "# Correlation between each property and popularity\n",
    "sb.pairplot(dataset, x_vars=usecols, y_vars=['popularity'])\n",
    "# The proprties we have do not directly correlate with the popularity of a song\n",
    "\n",
    "# Analyze binary and discrete features (mode, explicit, key)\n",
    "sb.displot(data=dataset, x='mode')        # more songs are in major key\n",
    "sb.displot(data=dataset, x='explicit')    # most songs are non explicit, most likely do not need this feature\n",
    "sb.displot(data=dataset, x='key')         # somewhat even distribution of keys\n",
    "\n",
    "# Analyze features in relation to popularity; these graphs are not very useful\n",
    "sb.pairplot(dataset, y_vars=['mode', 'explicit', 'key'], x_vars=['popularity'])\n",
    "\n",
    "# Count non-continuous features\n",
    "non_explicit = dataset[dataset['explicit'] == 0].shape[0]\n",
    "major = dataset[dataset['mode'] == 1].shape[0]\n",
    "total_songs = dataset.shape[0]\n",
    "print(f\"Non-Explicit songs / total songs = {non_explicit/total_songs}\")\n",
    "print(f\"Major key songs / total songs = {major/total_songs}\")\n",
    "\n",
    "# Analyze range of popularity of songs given explicit = 0 or 1 and mode = 0 or 1\n",
    "sb.histplot(data=dataset, x=\"popularity\", hue=\"explicit\", multiple=\"dodge\")\n",
    "# sb.histplot(data=dataset, x=\"popularity\", hue=\"mode\", multiple=\"dodge\")    # similar shapes for both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: further preprocessing?\n",
    "\n",
    "y = dataset.pop('popularity') # popularity is our class to predict\n",
    "X_headers = list(dataset.columns.values)\n",
    "X = dataset\n",
    "\n",
    "# Create the under sampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "# apply the transform\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "tempo_transformer = ReplaceZeroTransformer()\n",
    "X = tempo_transformer.transform(X)\n",
    "\n",
    "# need to scale after to treat the individual categories as their own class for the undersampling\n",
    "y = y/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the keys since they are a multiclass\n",
    "ohe = OneHotEncoder(categories='auto', drop='first')\n",
    "\n",
    "feature_arr = ohe.fit_transform(X[['key']]).toarray()\n",
    "columns_key = ['key_'+str(i) for i in list(set(X['key'].values))[1:]]\n",
    "features = pd.DataFrame(feature_arr, columns = columns_key, index = X.index)\n",
    "X = pd.concat([X, features], axis=1).drop(['key'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tuning hyper-parameters begin!\n",
      "\n",
      "Classifiers established, training data\n",
      "\n",
      "n_iterations: 9\n",
      "n_required_iterations: 10\n",
      "n_possible_iterations: 9\n",
      "min_resources_: 10\n",
      "max_resources_: 132427\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 37587\n",
      "n_resources: 10\n",
      "Fitting 5 folds for each of 37587 candidates, totalling 187935 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 12529\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 12529 candidates, totalling 62645 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 4177\n",
      "n_resources: 90\n",
      "Fitting 5 folds for each of 4177 candidates, totalling 20885 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 1393\n",
      "n_resources: 270\n",
      "Fitting 5 folds for each of 1393 candidates, totalling 6965 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 465\n",
      "n_resources: 810\n",
      "Fitting 5 folds for each of 465 candidates, totalling 2325 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 155\n",
      "n_resources: 2430\n",
      "Fitting 5 folds for each of 155 candidates, totalling 775 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 52\n",
      "n_resources: 7290\n",
      "Fitting 5 folds for each of 52 candidates, totalling 260 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 18\n",
      "n_resources: 21870\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 6\n",
      "n_resources: 65610\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Best parameters found: {'ccp_alpha': 3.938775510204081e-05, 'min_samples_split': 238}\n",
      "Score (mean squared): 0.025458861167044627\n",
      "Running cross validation\n",
      "\n",
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 out of   5 | elapsed:    1.8s remaining:    2.7s\n",
      "[Parallel(n_jobs=3)]: Done   5 out of   5 | elapsed:    3.3s finished\n",
      "Cross validation score (mean squared): 0.02543761576938327\n",
      "Feature importances:\n",
      "acousticness: 0.468\n",
      "danceability: 0.009\n",
      "duration_ms : 0.139\n",
      "energy      : 0.009\n",
      "explicit    : 0.144\n",
      "instrumentalness: 0.059\n",
      "liveness    : 0.012\n",
      "loudness    : 0.093\n",
      "mode        : 0.000\n",
      "speechiness : 0.047\n",
      "tempo       : 0.004\n",
      "valence     : 0.016\n",
      "key_1.0     : 0.000\n",
      "key_0.4545454545454546: 0.000\n",
      "key_0.18181818181818182: 0.000\n",
      "key_0.7272727272727273: 0.000\n",
      "key_0.36363636363636365: 0.000\n",
      "key_0.2727272727272727: 0.000\n",
      "key_0.8181818181818182: 0.000\n",
      "key_0.5454545454545454: 0.000\n",
      "key_0.0     : 0.000\n",
      "key_0.09090909090909091: 0.000\n",
      "key_0.6363636363636364: 0.000\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [\n",
    "    # {\n",
    "    #     'max_depth': np.arange(5, 15),\n",
    "    # },\n",
    "    {\n",
    "        'max_depth': np.arange(1, 20),\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "    },\n",
    "    {\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "        'max_leaf_nodes': np.arange(256, 324)\n",
    "    },\n",
    "    {\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "        'min_samples_split': np.arange(50, 700)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Available regression metrics are given here: https://scikit-learn.org/stable/modules/classes.html#regression-metric\n",
    "# https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions/42230764#42230764\n",
    "# ensure scikit is >0.18\n",
    "\n",
    "\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "print(\"Tuning hyper-parameters begin!\")\n",
    "print()\n",
    "\n",
    "# clf = GridSearchCV(tree.DecisionTreeRegressor(), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3)\n",
    "clf = HalvingGridSearchCV(tree.DecisionTreeRegressor(random_state=1), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3, random_state=1)\n",
    "print(\"Classifiers established, training data\")\n",
    "print()\n",
    "\n",
    "clf.fit(X, y)\n",
    "non_nested_scores = clf.best_score_\n",
    "print(\"Best parameters found:\", clf.best_params_)\n",
    "print(\"Score (mean squared):\", -clf.best_score_)\n",
    "\n",
    "print(\"Running cross validation\")\n",
    "print()\n",
    "clf.best_params_[\"random_state\"] = 1\n",
    "# cross_val_raw_data = cross_validate(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3, return_estimator=True, return_train_score=True)\n",
    "clf = tree.DecisionTreeRegressor(**clf.best_params_)\n",
    "cv_score = cross_val_score(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3,  scoring='neg_mean_squared_error')\n",
    "print(\"Cross validation score (mean squared):\", -cv_score.mean())\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "print('Feature importances:')\n",
    "for i, col in enumerate(X.columns):\n",
    "    print(f'{col:12}: {clf.feature_importances_[i]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report grid search results\n",
    "kwargs = {'ccp_alpha': 0.0003, 'criterion': 'mse', 'max_depth': 9, 'max_leaf_nodes': 260}\n",
    "clf = tree.DecisionTreeRegressor(**kwargs)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"r2: {clf.score(X_test, y_test)}\")\n",
    "print(f\"rmse: {mean_squared_error(y_test, clf.predict(X_test), squared=False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = [\"mse\", \"friedman_mse\"] # didn't make a big difference\n",
    "max_depth = 9 # found to result in best accuracy TODO: test over a range\n",
    "\n",
    "clf = tree.DecisionTreeRegressor(criterion=\"mse\", max_depth=max_depth)\n",
    "#     clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "# print(cross_val_score(clf, X_train, y_train, cv=cv))\n",
    "\n",
    "\n",
    "# Following a tutorial on Cost Complexity Pruning https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating different classifiers having different ccp_alpha values\n",
    "# WARNING: this takes a very long time to run! Below, ccp_alphas is pruned to cut down on computation time.\n",
    "# clfs = []\n",
    "# NUM_CCP_ALPHAS = 5\n",
    "# ccp_alphas = ccp_alphas[:NUM_CCP_ALPHAS]\n",
    "# for ccp_alpha in ccp_alphas:   \n",
    "#     print(f\"ccp_alpha: {ccp_alpha}\")\n",
    "#     clf = tree.DecisionTreeRegressor(random_state=0, ccp_alpha=ccp_alpha)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     clfs.append(clf)\n",
    "# print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "#       clfs[-1].tree_.node_count, ccp_alphas[-1]))\n",
    "\n",
    "# search for optimal ccp_alpha (seems to be somewhere in range 10**-4 to 10**-6)\n",
    "# 0 (99%/-0.6%), 0.0005 (34.3%/33.4%), 0.0000005 (88.3%/5.49%)\n",
    "for x in range(2,7):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, ccp_alpha=10**-x)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"alpha: {10**-x}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal tree depth (9)\n",
    "# TODO: plot different depths to demonstrate overfitting as depth increases past 9\n",
    "for md in range(2,25):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_depth=md)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"max_depth: {md}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal max leaf nodes value (maxima somewhere in (256, 324))\n",
    "for ln in range(2,25):\n",
    "    max_leaf_nodes = ln**2\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_leaf_nodes=max_leaf_nodes)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"max_leaf_node: {max_leaf_nodes}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal min no. leaf samples\n",
    "for msl in range(1,100,5):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_leaf_nodes=298, min_samples_leaf=msl)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"min_samples_leaf: {msl}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "print(train_scores)\n",
    "print(test_scores)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perform nested cross-validation (https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw tree\n",
    "clf.get_depth()\n",
    "plt.figure(figsize=(50,12))\n",
    "tree.plot_tree(clf,  fontsize=10, feature_names=headers)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}