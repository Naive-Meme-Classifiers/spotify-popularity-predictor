{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install directly into notebook\n",
    "%pip install sklearn\n",
    "%pip install pandas\n",
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider making list of dependencies for TA to install when running this notebook\n",
    "import sklearn as sk\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, KFold, cross_validate\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warning for chained assignment (not necessary but cleans up the project)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next 3 frames taken from kaggle example\n",
    "class ArtistsTransformer():\n",
    "  \"\"\" This transformer recives a DF with a feature 'artists' of dtype object\n",
    "      and convert the feature to a float value as follows:\n",
    "      1. Replace the data with the artists mean popularity\n",
    "      2. Replace values where artists appear less than MinCnt with y.mean()\n",
    "      3. Replace values where artists appear more than MaxCnt with 0\n",
    "      \n",
    "      PARAMETERS:\n",
    "      ----------\n",
    "      MinCnt (int): Minimal treshold of artisits apear in dataset, default = 3\n",
    "      MaxCnt (int): Maximal treshold of artisits apear in dataset, default = 600\n",
    "\n",
    "      RERTURN:\n",
    "      ----------\n",
    "      A DataFrame with converted artists str feature to ordinal floats\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, MinCnt = 3.0, MaxCnt = 600.0):\n",
    "      self.MinCnt = MinCnt\n",
    "      self.MaxCnt = MaxCnt\n",
    "      self.artists_df = None\n",
    "  \n",
    "  def fit (self, X, y):\n",
    "      self.artists_df =  y.groupby(X.artists).agg(['mean', 'count'])\n",
    "      self.artists_df.loc['unknown'] = [y.mean(), 1]\n",
    "      self.artists_df.loc[self.artists_df['count'] <= self.MinCnt, 'mean'] = y.mean()\n",
    "      self.artists_df.loc[self.artists_df['count'] >= self.MaxCnt, 'mean'] = 0\n",
    "      return self\n",
    "\n",
    "  def transform(self, X, y=None):\n",
    "      X['artists'] = np.where(X['artists'].isin(self.artists_df.index), X['artists'], 'unknown')\n",
    "      X['artists'] = X['artists'].map(self.artists_df['mean'])\n",
    "      return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instrumentalness_criteria(X):\n",
    "    X['instrumentalness'] = list(map((lambda x: 1 if x < 0.1 else (3 if x > 0.95 else 2)), X.instrumentalness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplaceZeroTransformer():\n",
    "    \"\"\"Eliminates Zero values from tempo columns and replace it \n",
    "       with the median or mean of non-zero values as specified.\n",
    "       defaut is set to 'median'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='median'):\n",
    "        self.method = method\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.method == 'median':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].median()\n",
    "        elif self.method == 'mean':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].mean()\n",
    "        else:\n",
    "            raise Exception(\"Method can be 'median' or 'mean' only!\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import track data\n",
    "usecols = ['acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode','popularity', 'speechiness', 'tempo', 'valence', 'artists']\n",
    "dataset = pd.read_csv(\"data.csv\", header = 0, usecols=usecols)\n",
    "\n",
    "# Remove rows duplicated by ignoring some columns\n",
    "dataset = dataset[~dataset.duplicated()==1]\n",
    "\n",
    "# Normalize columns having values outside [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "# cols_to_normalize = ['duration_ms', 'key', 'loudness', 'popularity', 'tempo']\n",
    "cols_to_normalize = ['duration_ms', 'key', 'loudness', 'tempo']\n",
    "dataset[cols_to_normalize] = scaler.fit_transform(dataset[cols_to_normalize])\n",
    "\n",
    "# print(dataset)\n",
    "\n",
    "y = dataset.pop('popularity') # popularity is our class to predict\n",
    "X_headers = list(dataset.columns.values)\n",
    "X = dataset\n",
    "\n",
    "# Create the under sampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "# apply the transform\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# need to scale after to treat the individual categories as their own class for the undersampling\n",
    "y = y/100\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Apply ArtistsTransformer\n",
    "artists_transformer = ArtistsTransformer(MinCnt=2)\n",
    "X_train = artists_transformer.fit(X_train, y_train).transform(X_train, y_train)\n",
    "X_test = artists_transformer.transform(X_test, y_test)\n",
    "\n",
    "# Instrumentalness Transformer\n",
    "instrumentalness_tranformer = FunctionTransformer(instrumentalness_criteria)\n",
    "instrumentalness_tranformer.transform(X_train)\n",
    "instrumentalness_tranformer.transform(X_test)\n",
    "\n",
    "# Tempo Transformer\n",
    "tempo_transformer = ReplaceZeroTransformer()\n",
    "X_train = tempo_transformer.transform(X_train)\n",
    "X_test = tempo_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [\n",
    "    # {\n",
    "    #     'max_depth': np.arange(5, 15),\n",
    "    # },\n",
    "    {\n",
    "        'max_depth': np.arange(1, 20),\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "    },\n",
    "    {\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "        'max_leaf_nodes': np.arange(256, 324)\n",
    "    },\n",
    "    {\n",
    "        'ccp_alpha' : np.append(0, np.linspace(0.000001, 0.0001, 50)),\n",
    "        'min_samples_split': np.arange(50, 700)\n",
    "    }\n",
    "]\n",
    "\n",
    "# Available regression metrics are given here: https://scikit-learn.org/stable/modules/classes.html#regression-metric\n",
    "# https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions/42230764#42230764\n",
    "# ensure scikit is >0.18\n",
    "\n",
    "non_nested_scores = np.zeros(NUM_TRIALS)\n",
    "nested_scores = np.zeros(NUM_TRIALS)\n",
    "\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "print(\"Tuning hyper-parameters begin!\")\n",
    "print()\n",
    "\n",
    "# clf = GridSearchCV(tree.DecisionTreeRegressor(), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3)\n",
    "clf = HalvingGridSearchCV(tree.DecisionTreeRegressor(random_state=1), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3, random_state=1)\n",
    "print(\"Classifiers established, training data\")\n",
    "print()\n",
    "\n",
    "clf.fit(X, y)\n",
    "non_nested_scores = clf.best_score_\n",
    "print(\"Best parameters found:\", clf.best_params_)\n",
    "print(\"Score (mean squared):\", -clf.best_score_)\n",
    "\n",
    "\n",
    "print(\"Running cross validation\")\n",
    "print()\n",
    "clf.best_params_[\"random_state\"] = 1\n",
    "# cross_val_raw_data = cross_validate(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3, return_estimator=True, return_train_score=True)\n",
    "clf = tree.DecisionTreeRegressor(**clf.best_params_)\n",
    "cv_score = cross_val_score(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3,  scoring='neg_mean_squared_error')\n",
    "print(\"Cross validation score (mean squared):\", -cv_score.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report grid search results\n",
    "kwargs = {'ccp_alpha': 0.0003, 'criterion': 'mse', 'max_depth': 9, 'max_leaf_nodes': 260}\n",
    "clf = tree.DecisionTreeRegressor(**kwargs)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"r2: {clf.score(X_test, y_test)}\")\n",
    "print(f\"rmse: {mean_squared_error(y_test, clf.predict(X_test), squared=False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = [\"mse\", \"friedman_mse\"] # didn't make a big difference\n",
    "max_depth = 9 # found to result in best accuracy TODO: test over a range\n",
    "\n",
    "clf = tree.DecisionTreeRegressor(criterion=\"mse\", max_depth=max_depth)\n",
    "#     clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "# print(cross_val_score(clf, X_train, y_train, cv=cv))\n",
    "\n",
    "\n",
    "# Following a tutorial on Cost Complexity Pruning https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating different classifiers having different ccp_alpha values\n",
    "# WARNING: this takes a very long time to run! Below, ccp_alphas is pruned to cut down on computation time.\n",
    "# clfs = []\n",
    "# NUM_CCP_ALPHAS = 5\n",
    "# ccp_alphas = ccp_alphas[:NUM_CCP_ALPHAS]\n",
    "# for ccp_alpha in ccp_alphas:   \n",
    "#     print(f\"ccp_alpha: {ccp_alpha}\")\n",
    "#     clf = tree.DecisionTreeRegressor(random_state=0, ccp_alpha=ccp_alpha)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     clfs.append(clf)\n",
    "# print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "#       clfs[-1].tree_.node_count, ccp_alphas[-1]))\n",
    "\n",
    "# search for optimal ccp_alpha (seems to be somewhere in range 10**-4 to 10**-6)\n",
    "# 0 (99%/-0.6%), 0.0005 (34.3%/33.4%), 0.0000005 (88.3%/5.49%)\n",
    "for x in range(2,7):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, ccp_alpha=10**-x)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"alpha: {10**-x}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal tree depth (9)\n",
    "# TODO: plot different depths to demonstrate overfitting as depth increases past 9\n",
    "for md in range(2,25):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_depth=md)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"max_depth: {md}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal max leaf nodes value (maxima somewhere in (256, 324))\n",
    "for ln in range(2,25):\n",
    "    max_leaf_nodes = ln**2\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_leaf_nodes=max_leaf_nodes)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"max_leaf_node: {max_leaf_nodes}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal min no. leaf samples\n",
    "for msl in range(1,100,5):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_leaf_nodes=298, min_samples_leaf=msl)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"min_samples_leaf: {msl}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "print(train_scores)\n",
    "print(test_scores)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perform nested cross-validation (https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw tree\n",
    "clf.get_depth()\n",
    "plt.figure(figsize=(50,12))\n",
    "tree.plot_tree(clf,  fontsize=10, feature_names=headers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}