{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in /home/robert/anaconda3/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/robert/anaconda3/lib/python3.8/site-packages (from sklearn) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/robert/.local/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.5.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /home/robert/.local/lib/python3.8/site-packages (1.1.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/robert/.local/lib/python3.8/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/robert/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/robert/.local/lib/python3.8/site-packages (from pandas) (1.19.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/robert/.local/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: imbalanced-learn in /home/robert/anaconda3/lib/python3.8/site-packages (0.8.0)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in /home/robert/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/robert/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/robert/anaconda3/lib/python3.8/site-packages (from imbalanced-learn) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/robert/.local/lib/python3.8/site-packages (from imbalanced-learn) (1.19.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/robert/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/home/robert/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install directly into notebook\n",
    "%pip install sklearn\n",
    "%pip install pandas\n",
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider making list of dependencies for TA to install when running this notebook\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warning for chained assignment (not necessary but cleans up the project)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next 3 frames taken from kaggle example\n",
    "class ArtistsTransformer():\n",
    "  \"\"\" This transformer recives a DF with a feature 'artists' of dtype object\n",
    "      and convert the feature to a float value as follows:\n",
    "      1. Replace the data with the artists mean popularity\n",
    "      2. Replace values where artists appear less than MinCnt with y.mean()\n",
    "      3. Replace values where artists appear more than MaxCnt with 0\n",
    "      \n",
    "      PARAMETERS:\n",
    "      ----------\n",
    "      MinCnt (int): Minimal treshold of artisits apear in dataset, default = 3\n",
    "      MaxCnt (int): Maximal treshold of artisits apear in dataset, default = 600\n",
    "\n",
    "      RERTURN:\n",
    "      ----------\n",
    "      A DataFrame with converted artists str feature to ordinal floats\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, MinCnt = 3.0, MaxCnt = 600.0):\n",
    "      self.MinCnt = MinCnt\n",
    "      self.MaxCnt = MaxCnt\n",
    "      self.artists_df = None\n",
    "  \n",
    "  def fit (self, X, y):\n",
    "      self.artists_df =  y.groupby(X.artists).agg(['mean', 'count'])\n",
    "      self.artists_df.loc['unknown'] = [y.mean(), 1]\n",
    "      self.artists_df.loc[self.artists_df['count'] <= self.MinCnt, 'mean'] = y.mean()\n",
    "      self.artists_df.loc[self.artists_df['count'] >= self.MaxCnt, 'mean'] = 0\n",
    "      return self\n",
    "\n",
    "  def transform(self, X, y=None):\n",
    "      X['artists'] = np.where(X['artists'].isin(self.artists_df.index), X['artists'], 'unknown')\n",
    "      X['artists'] = X['artists'].map(self.artists_df['mean'])\n",
    "      return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instrumentalness_criteria(X):\n",
    "    X['instrumentalness'] = list(map((lambda x: 1 if x < 0.1 else (3 if x > 0.95 else 2)), X.instrumentalness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplaceZeroTransformer():\n",
    "    \"\"\"Eliminates Zero values from tempo columns and replace it \n",
    "       with the median or mean of non-zero values as specified.\n",
    "       defaut is set to 'median'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='median'):\n",
    "        self.method = method\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.method == 'median':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].median()\n",
    "        elif self.method == 'mean':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].mean()\n",
    "        else:\n",
    "            raise Exception(\"Method can be 'median' or 'mean' only!\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        acousticness                                   artists  danceability  \\\n",
      "0           0.991000                           ['Mamie Smith']         0.598   \n",
      "1           0.643000                 [\"Screamin' Jay Hawkins\"]         0.852   \n",
      "2           0.993000                           ['Mamie Smith']         0.647   \n",
      "3           0.000173                       ['Oscar Velazquez']         0.730   \n",
      "4           0.295000                                  ['Mixe']         0.704   \n",
      "...              ...                                       ...           ...   \n",
      "174369      0.995000  ['Ludovico Einaudi', 'Johannes Bornlöf']         0.297   \n",
      "174371      0.995000  ['Ludovico Einaudi', 'Johannes Bornlöf']         0.343   \n",
      "174375      0.988000  ['Ludovico Einaudi', 'Johannes Bornlöf']         0.316   \n",
      "174377      0.795000                          ['Alessia Cara']         0.429   \n",
      "174387      0.920000                          ['Taylor Swift']         0.462   \n",
      "\n",
      "        duration_ms  energy  explicit  instrumentalness       key  liveness  \\\n",
      "0          0.030637  0.2240         0          0.000522  0.454545    0.3790   \n",
      "1          0.027237  0.5170         0          0.026400  0.454545    0.0809   \n",
      "2          0.029792  0.1860         0          0.000018  0.000000    0.5190   \n",
      "3          0.078215  0.7980         0          0.801000  0.181818    0.1280   \n",
      "4          0.030054  0.7070         1          0.000246  0.909091    0.4020   \n",
      "...             ...     ...       ...               ...       ...       ...   \n",
      "174369     0.064549  0.0287         0          0.908000  0.727273    0.0995   \n",
      "174371     0.037830  0.0165         0          0.878000  0.818182    0.0774   \n",
      "174375     0.055949  0.0573         0          0.879000  0.272727    0.1200   \n",
      "174377     0.026209  0.2110         0          0.000000  0.363636    0.1960   \n",
      "174387     0.044824  0.2400         1          0.000000  0.000000    0.1130   \n",
      "\n",
      "        loudness  mode  popularity  speechiness     tempo  valence  \n",
      "0       0.741868     0          12       0.0936  0.615900   0.6340  \n",
      "1       0.825918     0           7       0.0534  0.356823   0.9500  \n",
      "2       0.750168     1           4       0.1740  0.400810   0.6890  \n",
      "3       0.825135     1          17       0.0425  0.525640   0.0422  \n",
      "4       0.845102     0           2       0.0768  0.501324   0.2990  \n",
      "...          ...   ...         ...          ...       ...      ...  \n",
      "174369  0.469689     1           0       0.0564  0.581651   0.0678  \n",
      "174371  0.455485     0           0       0.0455  0.521422   0.1510  \n",
      "174375  0.561882     1           0       0.0515  0.332927   0.0373  \n",
      "174377  0.756949     1           0       0.0360  0.388942   0.2280  \n",
      "174387  0.750497     1          69       0.0377  0.703549   0.3200  \n",
      "\n",
      "[170533 rows x 15 columns]\n"
     ]
    }
   ],
   "source": [
    "# Import track data\n",
    "usecols = ['acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode','popularity', 'speechiness', 'tempo', 'valence', 'artists']\n",
    "dataset = pd.read_csv(\"data.csv\", header = 0, usecols=usecols)\n",
    "\n",
    "# Remove rows duplicated by ignoring some columns\n",
    "dataset = dataset[~dataset.duplicated()==1]\n",
    "\n",
    "# Normalize columns having values outside [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "# cols_to_normalize = ['duration_ms', 'key', 'loudness', 'popularity', 'tempo']\n",
    "cols_to_normalize = ['duration_ms', 'key', 'loudness', 'tempo']\n",
    "dataset[cols_to_normalize] = scaler.fit_transform(dataset[cols_to_normalize])\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "y = dataset.pop('popularity') # popularity is our class to predict\n",
    "X_headers = list(dataset.columns.values)\n",
    "X = dataset\n",
    "\n",
    "# Create the under sampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "# apply the transform\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "# need to scale after to treat the individual categories as their own class for the undersampling\n",
    "y = y/100\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Apply ArtistsTransformer\n",
    "artists_transformer = ArtistsTransformer(MinCnt=2)\n",
    "X_train = artists_transformer.fit(X_train, y_train).transform(X_train, y_train)\n",
    "X_test = artists_transformer.transform(X_test, y_test)\n",
    "\n",
    "# Instrumentalness Transformer\n",
    "instrumentalness_tranformer = FunctionTransformer(instrumentalness_criteria)\n",
    "instrumentalness_tranformer.transform(X_train)\n",
    "instrumentalness_tranformer.transform(X_test)\n",
    "\n",
    "# Tempo Transformer\n",
    "tempo_transformer = ReplaceZeroTransformer()\n",
    "X_train = tempo_transformer.transform(X_train)\n",
    "X_test = tempo_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-136-cf40091e941e>, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-136-cf40091e941e>\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    for score in scores:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{\n",
    "    'criterion': ['mse'],\n",
    "    'max_depth': np.linspace(1, 10, 10, dtype=int),\n",
    "    'ccp_alpha' : np.linspace(0.0003, 0.0006, 6),\n",
    "    'max_leaf_nodes': np.linspace(256, 324, 35, dtype=int)\n",
    "    }]\n",
    "\n",
    "# Available regression metrics are given here: https://scikit-learn.org/stable/modules/classes.html#regression-metrics\n",
    "scores = ['r2_score', # 'neg_mean_squared_error', 'max_error'] \n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        tree.DecisionTreeRegressor(), tuned_parameters, scoring=score\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "#     print(\"Detailed classification report:\")\n",
    "#     print()\n",
    "#     print(\"The model is trained on the full development set.\")\n",
    "#     print(\"The scores are computed on the full evaluation set.\")\n",
    "#     print()\n",
    "#     y_true, y_pred = y_test, clf.predict(X_test)\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2: 0.4560733340519424\n",
      "rmse: 0.1401389371544769\n"
     ]
    }
   ],
   "source": [
    "# Report grid search results\n",
    "kwargs = {'ccp_alpha': 0.0003, 'criterion': 'mse', 'max_depth': 9, 'max_leaf_nodes': 260}\n",
    "clf = tree.DecisionTreeRegressor(**kwargs)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"r2: {clf.score(X_test, y_test)}\")\n",
    "print(f\"rmse: {mean_squared_error(y_test, clf.predict(X_test), squared=False)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = [\"mse\", \"friedman_mse\"] # didn't make a big difference\n",
    "max_depth = 9 # found to result in best accuracy TODO: test over a range\n",
    "\n",
    "clf = tree.DecisionTreeRegressor(criterion=\"mse\", max_depth=max_depth)\n",
    "#     clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "# print(cross_val_score(clf, X_train, y_train, cv=cv))\n",
    "\n",
    "\n",
    "# Following a tutorial on Cost Complexity Pruning https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating different classifiers having different ccp_alpha values\n",
    "# WARNING: this takes a very long time to run! Below, ccp_alphas is pruned to cut down on computation time.\n",
    "# clfs = []\n",
    "# NUM_CCP_ALPHAS = 5\n",
    "# ccp_alphas = ccp_alphas[:NUM_CCP_ALPHAS]\n",
    "# for ccp_alpha in ccp_alphas:   \n",
    "#     print(f\"ccp_alpha: {ccp_alpha}\")\n",
    "#     clf = tree.DecisionTreeRegressor(random_state=0, ccp_alpha=ccp_alpha)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     clfs.append(clf)\n",
    "# print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "#       clfs[-1].tree_.node_count, ccp_alphas[-1]))\n",
    "\n",
    "# search for optimal ccp_alpha (seems to be somewhere in range 10**-4 to 10**-6)\n",
    "# 0 (99%/-0.6%), 0.0005 (34.3%/33.4%), 0.0000005 (88.3%/5.49%)\n",
    "for x in range(2,7):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, ccp_alpha=10**-x)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"alpha: {10**-x}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal tree depth (9)\n",
    "# TODO: plot different depths to demonstrate overfitting as depth increases past 9\n",
    "for md in range(2,25):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_depth=md)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"max_depth: {md}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal max leaf nodes value (maxima somewhere in (256, 324))\n",
    "for ln in range(2,25):\n",
    "    max_leaf_nodes = ln**2\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_leaf_nodes=max_leaf_nodes)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"max_leaf_node: {max_leaf_nodes}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal min no. leaf samples\n",
    "for msl in range(1,100,5):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_leaf_nodes=298, min_samples_leaf=msl)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"min_samples_leaf: {msl}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "print(train_scores)\n",
    "print(test_scores)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perform nested cross-validation (https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw tree\n",
    "clf.get_depth()\n",
    "plt.figure(figsize=(50,12))\n",
    "tree.plot_tree(clf,  fontsize=10, feature_names=headers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
