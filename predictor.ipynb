{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider making list of dependencies for TA to install when running this notebook\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-outdoors",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import track data\n",
    "usecols = ['acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode','popularity', 'speechiness', 'tempo', 'valence']\n",
    "dataset = pd.read_csv(\"data.csv\", header = 0, usecols=usecols)\n",
    "\n",
    "# Remove rows duplicated by ignoring some columns\n",
    "dataset = dataset[~dataset.duplicated()==1]\n",
    "\n",
    "# Normalize columns having values outside [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "cols_to_normalize = ['duration_ms', 'key', 'loudness', 'popularity', 'tempo']\n",
    "dataset[cols_to_normalize] = scaler.fit_transform(dataset[cols_to_normalize])\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-uncle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data analysis\n",
    "# pair plots: https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "import seaborn as sb\n",
    "\n",
    "# Pairwise plots between each column in the data set excluding popularity, binary (mode and explicit) and discrete (key) features\n",
    "pair_plot_data = dataset.drop(columns=['mode', 'explicit', 'key', 'popularity'])\n",
    "sb.pairplot(pair_plot_data, corner=True)\n",
    "# There isn't a lot of direct correlation between any two property of a song\n",
    "\n",
    "# Correlation between each property and popularity\n",
    "sb.pairplot(dataset, x_vars=usecols, y_vars=['popularity'])\n",
    "# The proprties we have do not directly correlate with the popularity of a song\n",
    "\n",
    "# Analyze binary and discrete features (mode, explicit, key)\n",
    "sb.displot(data=dataset, x='mode')        # more songs are in major key\n",
    "sb.displot(data=dataset, x='explicit')    # most songs are non explicit, most likely do not need this feature\n",
    "sb.displot(data=dataset, x='key')         # somewhat even distribution of keys\n",
    "\n",
    "# Analyze features in relation to popularity; these graphs are not very useful\n",
    "sb.pairplot(dataset, y_vars=['mode', 'explicit', 'key'], x_vars=['popularity'])\n",
    "\n",
    "# Count non-continuous features\n",
    "non_explicit = dataset[dataset['explicit'] == 0].shape[0]\n",
    "major = dataset[dataset['mode'] == 1].shape[0]\n",
    "total_songs = dataset.shape[0]\n",
    "print(f\"Non-Explicit songs / total songs = {non_explicit/total_songs}\")\n",
    "print(f\"Major key songs / total songs = {major/total_songs}\")\n",
    "\n",
    "# Analyze range of popularity of songs given explicit = 0 or 1 and mode = 0 or 1\n",
    "sb.histplot(data=dataset, x=\"popularity\", hue=\"explicit\", multiple=\"dodge\")\n",
    "# sb.histplot(data=dataset, x=\"popularity\", hue=\"mode\", multiple=\"dodge\")    # similar shapes for both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-princess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: further preprocessing?\n",
    "\n",
    "y = dataset.pop('popularity') # popularity is our class to predict\n",
    "X_headers = list(dataset.columns.values)\n",
    "X = dataset.to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ultimate-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{\n",
    "    'criterion': ['mse'],\n",
    "    'max_depth': np.linspace(1, 10, 10, dtype=int),\n",
    "    'ccp_alpha' : np.linspace(0.0003, 0.0006, 6),\n",
    "    'max_leaf_nodes': np.linspace(256, 324, 35, dtype=int)\n",
    "    }]\n",
    "\n",
    "# Available regression metrics are given here: https://scikit-learn.org/stable/modules/classes.html#regression-metrics\n",
    "scores = ['r2_score', # 'neg_mean_squared_error', 'max_error'] \n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(\n",
    "        tree.DecisionTreeRegressor(), tuned_parameters, scoring=score\n",
    "    )\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "#     print(\"Detailed classification report:\")\n",
    "#     print()\n",
    "#     print(\"The model is trained on the full development set.\")\n",
    "#     print(\"The scores are computed on the full evaluation set.\")\n",
    "#     print()\n",
    "#     y_true, y_pred = y_test, clf.predict(X_test)\n",
    "#     print(classification_report(y_true, y_pred))\n",
    "#     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-surface",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report grid search results\n",
    "kwargs = {'ccp_alpha': 0.0003, 'criterion': 'mse', 'max_depth': 6, 'max_leaf_nodes': 260}\n",
    "clf = tree.DecisionTreeRegressor(**kwargs)\n",
    "clf.fit(X_train, y_train)\n",
    "print(f\"r2: {clf.score(X_test, y_test)}\")\n",
    "print(f\"mse: {mean_squared_error(y_test, clf.predict(X_test), squared=True)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "racial-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = [\"mse\", \"friedman_mse\"] # didn't make a big difference\n",
    "max_depth = 9 # found to result in best accuracy TODO: test over a range\n",
    "\n",
    "clf = tree.DecisionTreeRegressor(criterion=\"mse\", max_depth=max_depth)\n",
    "#     clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# cv = ShuffleSplit(n_splits=5, test_size=0.3, random_state=0)\n",
    "# print(cross_val_score(clf, X_train, y_train, cv=cv))\n",
    "\n",
    "\n",
    "# Following a tutorial on Cost Complexity Pruning https://scikit-learn.org/stable/auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py\n",
    "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating different classifiers having different ccp_alpha values\n",
    "# WARNING: this takes a very long time to run! Below, ccp_alphas is pruned to cut down on computation time.\n",
    "# clfs = []\n",
    "# NUM_CCP_ALPHAS = 5\n",
    "# ccp_alphas = ccp_alphas[:NUM_CCP_ALPHAS]\n",
    "# for ccp_alpha in ccp_alphas:   \n",
    "#     print(f\"ccp_alpha: {ccp_alpha}\")\n",
    "#     clf = tree.DecisionTreeRegressor(random_state=0, ccp_alpha=ccp_alpha)\n",
    "#     clf.fit(X_train, y_train)\n",
    "#     clfs.append(clf)\n",
    "# print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "#       clfs[-1].tree_.node_count, ccp_alphas[-1]))\n",
    "\n",
    "# search for optimal ccp_alpha (seems to be somewhere in range 10**-4 to 10**-6)\n",
    "# 0 (99%/-0.6%), 0.0005 (34.3%/33.4%), 0.0000005 (88.3%/5.49%)\n",
    "for x in range(2,7):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, ccp_alpha=10**-x)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"alpha: {10**-x}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal tree depth (9)\n",
    "# TODO: plot different depths to demonstrate overfitting as depth increases past 9\n",
    "for md in range(2,25):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_depth=md)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"max_depth: {md}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-pencil",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal max leaf nodes value (maxima somewhere in (256, 324))\n",
    "for ln in range(2,25):\n",
    "    max_leaf_nodes = ln**2\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_leaf_nodes=max_leaf_nodes)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"max_leaf_node: {max_leaf_nodes}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-assignment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for optimal min no. leaf samples\n",
    "for msl in range(1,100,5):\n",
    "    clf = tree.DecisionTreeRegressor(random_state=0, max_leaf_nodes=298, min_samples_leaf=msl)\n",
    "    clf.fit(X_train, y_train)\n",
    "    print(f\"min_samples_leaf: {msl}\")\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-canon",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "print(train_scores)\n",
    "print(test_scores)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: perform nested cross-validation (https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-highlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw tree\n",
    "clf.get_depth()\n",
    "plt.figure(figsize=(50,12))\n",
    "tree.plot_tree(clf,  fontsize=10, feature_names=headers)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-worker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparable-hometown",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
