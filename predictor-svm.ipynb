{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install directly into notebook\n",
    "%pip install sklearn\n",
    "%pip install pandas\n",
    "%pip install imbalanced-learn\n",
    "%pip install seaborn\n",
    "%pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: consider making list of dependencies for TA to install when running this notebook\n",
    "import sklearn as sk\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, ShuffleSplit, KFold, cross_validate\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# disable warning for chained assignment (not necessary but cleans up the project)\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from kaggle example\n",
    "class ReplaceZeroTransformer():\n",
    "    \"\"\"Eliminates Zero values from tempo columns and replace it \n",
    "       with the median or mean of non-zero values as specified.\n",
    "       defaut is set to 'median'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, method='median'):\n",
    "        self.method = method\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self.method == 'median':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].median()\n",
    "        elif self.method == 'mean':\n",
    "            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].mean()\n",
    "        else:\n",
    "            raise Exception(\"Method can be 'median' or 'mean' only!\")\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-recorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import track data\n",
    "usecols = ['acousticness', 'danceability', 'duration_ms', 'energy', 'explicit', 'instrumentalness', 'key', 'liveness', 'loudness', 'mode','popularity', 'speechiness', 'tempo', 'valence']\n",
    "dataset = pd.read_csv(\"data.csv\", header = 0, usecols=usecols)\n",
    "\n",
    "# Remove rows duplicated by ignoring some columns\n",
    "dataset = dataset[~dataset.duplicated()==1]\n",
    "\n",
    "# Normalize columns having values outside [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "# cols_to_normalize = ['duration_ms', 'key', 'loudness', 'popularity', 'tempo']\n",
    "cols_to_normalize = ['duration_ms', 'loudness', 'tempo']\n",
    "dataset[cols_to_normalize] = scaler.fit_transform(dataset[cols_to_normalize])\n",
    "\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-pierre",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Data analysis\n",
    "# pair plots: https://seaborn.pydata.org/generated/seaborn.pairplot.html\n",
    "import seaborn as sb\n",
    "\n",
    "# Pairwise plots between each column in the data set excluding popularity, binary (mode and explicit) and discrete (key) features\n",
    "pair_plot_data = dataset.drop(columns=['mode', 'explicit', 'key', 'popularity'])\n",
    "sb.pairplot(pair_plot_data, corner=True)\n",
    "# There isn't a lot of direct correlation between any two property of a song\n",
    "\n",
    "# Correlation between each property and popularity\n",
    "sb.pairplot(dataset, x_vars=usecols, y_vars=['popularity'])\n",
    "# The proprties we have do not directly correlate with the popularity of a song\n",
    "\n",
    "# Analyze binary and discrete features (mode, explicit, key)\n",
    "sb.displot(data=dataset, x='mode')        # more songs are in major key\n",
    "sb.displot(data=dataset, x='explicit')    # most songs are non explicit, most likely do not need this feature\n",
    "sb.displot(data=dataset, x='key')         # somewhat even distribution of keys\n",
    "\n",
    "# Analyze features in relation to popularity; these graphs are not very useful\n",
    "sb.pairplot(dataset, y_vars=['mode', 'explicit', 'key'], x_vars=['popularity'])\n",
    "\n",
    "# Count non-continuous features\n",
    "non_explicit = dataset[dataset['explicit'] == 0].shape[0]\n",
    "major = dataset[dataset['mode'] == 1].shape[0]\n",
    "total_songs = dataset.shape[0]\n",
    "print(f\"Non-Explicit songs / total songs = {non_explicit/total_songs}\")\n",
    "print(f\"Major key songs / total songs = {major/total_songs}\")\n",
    "\n",
    "# Analyze range of popularity of songs given explicit = 0 or 1 and mode = 0 or 1\n",
    "sb.histplot(data=dataset, x=\"popularity\", hue=\"explicit\", multiple=\"dodge\")\n",
    "# sb.histplot(data=dataset, x=\"popularity\", hue=\"mode\", multiple=\"dodge\")    # similar shapes for both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-developer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: further preprocessing?\n",
    "\n",
    "y = dataset.pop('popularity') # popularity is our class to predict\n",
    "X_headers = list(dataset.columns.values)\n",
    "X = dataset\n",
    "\n",
    "# Create the under sampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "\n",
    "# apply the transform\n",
    "X, y = undersample.fit_resample(X, y)\n",
    "\n",
    "tempo_transformer = ReplaceZeroTransformer()\n",
    "X = tempo_transformer.transform(X)\n",
    "\n",
    "# need to scale after to treat the individual categories as their own class for the undersampling\n",
    "y = y/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-preserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the keys since they are a multiclass\n",
    "ohe = OneHotEncoder(categories='auto', drop='first')\n",
    "\n",
    "feature_arr = ohe.fit_transform(X[['key']]).toarray()\n",
    "columns_key = ['key_'+str(i) for i in list(set(X['key'].values))[1:]]\n",
    "features = pd.DataFrame(feature_arr, columns = columns_key, index = X.index)\n",
    "X = pd.concat([X, features], axis=1).drop(['key'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [\n",
    "    # {\n",
    "    #     'max_depth': np.arange(5, 15),\n",
    "    # },\n",
    "    # {\n",
    "    #     'kernel': [\"linear\"],\n",
    "    # },\n",
    "    {\n",
    "        'kernel': [\"rbf\"],\n",
    "        'C': np.linspace(0.1, 1, 5)\n",
    "    },\n",
    "        # {\n",
    "        # 'kernel': [\"sigmoid\"],\n",
    "        # 'coef0': np.arange(0, 5),\n",
    "        # 'C': np.linspace(0.1, 1, 5)\n",
    "    # },\n",
    "    {\n",
    "        'kernel': [\"poly\"],\n",
    "        'coef0': np.arange(0, 5),\n",
    "        'degree': np.arange(3, 5),\n",
    "        'C': np.linspace(0.1, 1, 5),\n",
    "    },\n",
    "]\n",
    "\n",
    "# Available regression metrics are given here: https://scikit-learn.org/stable/modules/classes.html#regression-metric\n",
    "# https://stackoverflow.com/questions/42228735/scikit-learn-gridsearchcv-with-multiple-repetitions/42230764#42230764\n",
    "# ensure scikit is >0.18\n",
    "\n",
    "\n",
    "inner_cv = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "outer_cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "print(\"Tuning hyper-parameters begin!\")\n",
    "print()\n",
    "\n",
    "# clf = GridSearchCV(tree.DecisionTreeRegressor(), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3)\n",
    "clf = HalvingGridSearchCV(svm.SVR(verbose=True,cache_size=3000), tuned_parameters, cv=inner_cv, scoring='neg_mean_squared_error', verbose=4, n_jobs=3, random_state=1)\n",
    "print(\"Classifiers established, training data\")\n",
    "print()\n",
    "\n",
    "clf.fit(X, y)\n",
    "non_nested_scores = clf.best_score_\n",
    "print(\"Best parameters found:\", clf.best_params_)\n",
    "print(\"Score (mean squared):\", -clf.best_score_)\n",
    "\n",
    "print(\"Running cross validation\")\n",
    "print()\n",
    "clf.best_params_[\"cache_size\"] = 3000\n",
    "clf.best_params_[\"verbose\"] = True\n",
    "\n",
    "# cross_val_raw_data = cross_validate(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3, return_estimator=True, return_train_score=True)\n",
    "clf = svm.SVR(**clf.best_params_)\n",
    "cv_score = cross_val_score(clf, X=X, y=y, cv=outer_cv, verbose=4,  n_jobs=3,  scoring='neg_mean_squared_error')\n",
    "print(\"Cross validation score (mean squared):\", -cv_score.mean())\n",
    "\n",
    "clf.fit(X, y)\n",
    "\n",
    "# print('Feature importances:')\n",
    "# for i, col in enumerate(X.columns):\n",
    "#     print(f'{col:18}: {clf.feature_importances_[i]:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the KernelExplainer example available at https://github.com/slundberg/shap\n",
    "shap.initjs()\n",
    "hyperparameters = {'cache_size'=3000, 'C': 0.325, 'coef0': 3, 'degree': 3, 'kernel': 'poly'}\n",
    "svm = svm.SVR(**hyperparameters)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "explainer = shap.KernelExplainer(svm.predict_proba, X_train, link=\"logit\")\n",
    "shap_values = explainer.shap_values(X_test, nsamples=100)\n",
    "\n",
    "# plot the SHAP values for the Setosa output of the first instance\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link=\"logit\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
